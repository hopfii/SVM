
@article{burges_tutorial_1998,
	title = {A Tutorial on Support Vector Machines for Pattern Recognition},
	volume = {2},
	issn = {1573-756X},
	url = {https://doi.org/10.1023/A:1009715923555},
	doi = {10.1023/A:1009715923555},
	abstract = {The tutorial starts with an overview of the concepts of {VC} dimension and structural risk minimization. We then describe linear Support Vector Machines ({SVMs}) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when {SVM} solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct {SVM} solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) {VC} dimension by computing the {VC} dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high {VC} dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for {SVMs}, there are several arguments which support the observed high accuracy of {SVMs}, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
	pages = {121--167},
	number = {2},
	journaltitle = {Data Mining and Knowledge Discovery},
	shortjournal = {Data Mining and Knowledge Discovery},
	author = {Burges, Christopher J.C.},
	urldate = {2021-03-06},
	date = {1998-06-01},
	langid = {english}
}

@report{platt_sequential_1998,
	title = {Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines},
	url = {https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/},
	abstract = {This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization , or {SMO} . Training a support vector machine requires the solution of a very large quadratic programming ({QP}) optimization problem. {SMO} breaks this large {QP} problem into a series of smallest possible {QP} problems. These small {QP} problems are solved analytically, which avoids using a time-consuming numerical {QP} optimization as an inner loop. The amount of memory required for {SMO} is linear in the training set size, which allows {SMO} to handle very large training sets. Because matrix computation is avoided, {SMO} scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking {SVM} algorithm scales somewhere between linear and cubic in the training set size. {SMO}'s computation time is dominated by {SVM} evaluation, hence {SMO} is fastest for linear {SVMs} and sparse data sets. On real- world sparse data sets, {SMO} can be more than 1000 times faster than the chunking algorithm.},
	pages = {21},
	number = {{MSR}-{TR}-98-14},
	author = {Platt, John},
	date = {1998-04}
}

@book{abu-mostafa_learning_2012,
	title = {Learning From Data},
	isbn = {978-1-60049-006-4},
	abstract = {Machine learning allows computational systems to adaptively improve their performance with experience accumulated from the observed data. Its techniques are widely applied in engineering, science, finance, and commerce. This book is designed for a short course on machine learning. It is a short course, not a hurried course. From over a decade of teaching this material, we have distilled what we believe to be the core topics that every student of the subject should know. We chose the title `learning from data' that faithfully describes what the subject is about, and made it a point to cover the topics in a story-like fashion. Our hope is that the reader can learn all the fundamentals of the subject by reading the book cover to cover. ---- Learning from data has distinct theoretical and practical tracks. In this book, we balance the theoretical and the practical, the mathematical and the heuristic. Our criterion for inclusion is relevance. Theory that establishes the conceptual framework for learning is included, and so are heuristics that impact the performance of real learning systems. ---- Learning from data is a very dynamic field. Some of the hot techniques and theories at times become just fads, and others gain traction and become part of the field. What we have emphasized in this book are the necessary fundamentals that give any student of learning from data a solid foundation, and enable him or her to venture out and explore further techniques and theories, or perhaps to contribute their own. ---- The authors are professors at California Institute of Technology (Caltech), Rensselaer Polytechnic Institute ({RPI}), and National Taiwan University ({NTU}), where this book is the main text for their popular courses on machine learning. The authors also consult extensively with financial and commercial companies on machine learning applications, and have led winning teams in machine learning competitions.},
	pagetotal = {213},
	publisher = {{AMLBook}},
	author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
	date = {2012}
}

@unpublished{winston_6034_2010,
	location = {Massachusetts Institute of Technology: {MIT} {OpenCourseWare}},
	title = {6.034 Artificial Intelligence},
	rights = {Creative Commons {BY}-{NC}-{SA}},
	url = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010},
	author = {Winston, Patrick},
	date = {2010}
}

@book{boyd_convex_2004,
	location = {Cambridge, {UK} ; New York},
	title = {Convex Optimization},
	isbn = {978-0-521-83378-3},
	url = {https://web.stanford.edu/~boyd/cvxbook/},
	abstract = {Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.},
	pagetotal = {727},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	date = {2004}
}

@book{kecman_learning_2001,
	edition = {Reprint Edition},
	title = {Learning and Soft Computing: Support Vector Machines, Neural Networks, and Fuzzy Logic Models},
	isbn = {978-0-262-52790-3},
	shorttitle = {Learning and Soft Computing},
	abstract = {This textbook provides a thorough introduction to the field of learning from experimental data and soft computing. Support vector machines ({SVM}) and neural networks ({NN}) are the mathematical structures, or models, that underlie learning, while fuzzy logic systems ({FLS}) enable us to embed structured human knowledge into workable algorithms. The book assumes that it is not only useful, but necessary, to treat {SVM}, {NN}, and {FLS} as parts of a connected whole. Throughout, the theory and algorithms are illustrated by practical examples, as well as by problem sets and simulated experiments. This approach enables the reader to develop {SVM}, {NN}, and {FLS} in addition to understanding them. The book also presents three case studies: on {NN}-based control, financial time series analysis, and computer graphics. A solutions manual and all of the {MATLAB} programs needed for the simulated experiments are available.},
	pagetotal = {576},
	publisher = {{MIT} Press},
	author = {Kecman, Vojislav},
	date = {2001-06-08}
}

@book{bishop_pattern_2016,
	location = {New York, {NY}},
	edition = {Softcover reprint of the original 1st ed. 2006 Edition},
	title = {Pattern Recognition and Machine Learning},
	isbn = {978-1-4939-3843-8},
	abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
	pagetotal = {758},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	date = {2016-08-23}
}